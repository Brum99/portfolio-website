import CustomImage from '@/components/CustomImage'
import ModelButton from '@/components/ModelButton'




<div className = "w-1/2 mx-auto text-center mb-5">
# Comparative Analysis of CNN Architectures for Brain Tumour Classification

  
> Explore the complete implementation: [GitHub Repository](https://github.com/Brum99/MRI_brain_tumor_prediction)
</div>
---


<div className="mt-10 mb-10">
    <div className="mb-5">
    Why This Matters
    </div>

    Before diving into the technical implementation of this project, it's essential
    to understand **why this work matters**. Brain tumor detection is not just a 
    computational challenge, it’s a life critical task with real world 
    implications. The types of tumors discussed here differ dramatically in 
    prognosis, treatment approach, and urgency. By establishing a clear clinical 
    picture of these conditions, we can better appreciate the role that machine 
    learning can play in **supporting early, accurate, and scalable diagnosis**.
</div>

---

<div className="mt-10 mb-10">

    <div className="mb-5">
    Clinical Challenge: The Need for Automated Diagnostics
    </div>

    Brain tumors represent one of the most critical diagnostic challenges in modern medicine. These abnormal cell proliferations can be benign or malignant, with early detection being paramount for successful treatment outcomes and patient survival rates.

    <div className="mt-5 mb-5">
    While Magnetic Resonance Imaging (MRI) provides exceptional soft tissue contrast for tumor visualisation, **the manual interpretation process faces significant bottlenecks**:
    </div>

    <div className="mt-5 mb-5">
    - **Time Constraints**: Radiologists may review 100+ scans daily, leading to fatigue-induced oversights
    - **Diagnostic Variability**: Inter-observer agreement rates can vary significantly between practitioners  
    - **Resource Limitations**: Many healthcare systems face critical shortages of specialised radiologists
    - **Urgent Triage Needs**: Emergency cases require rapid preliminary assessments
    </div>


    This project investigates how **Convolutional Neural Networks (CNNs)** can serve as intelligent diagnostic support tools, providing rapid preliminary assessments while maintaining the essential human expertise in final diagnosis.

    <div className="mt-5 mb-5">
    According to the **World Health Organisation (WHO)**, a full clinical diagnosis of brain tumors includes identifying not only the tumor’s type but also its **grade** and **malignancy** level.  
In this project, we focus **only on classifying the type of tumor** based on MRI features, not on grading or malignancy.
    </div>
This is a crucial step in building automated systems that could eventually aid in faster and more scalable triage, particularly in **resource-limited settings**.

</div>

---

<div className="mt-10 mb-10">
## Tumor Types in This Study

<div className="flex flex-col md:flex-row gap-10 mt-6 items-start">

  {/* LEFT: Tumor Descriptions */}
  <div className="flex flex-col space-y-10 md:w-1/2">
    <div className="space-y-2">
      <h3 className="text-2xl font-semibold">
        1. Pituitary Adenomas <span className="text-green-600 text-lg">(Often benign & highly treatable)</span>
      </h3>
      <p><strong>Location:</strong> Sellar and suprasellar regions, the “master gland” area behind the eyes.</p>
      <p><strong>Characteristics:</strong> Benign tumors from the pituitary gland, often hormonally active.</p>
      <p><strong>Clinical Impact:</strong> Can lead to <em>Cushing’s disease</em>, vision loss, mood swings, and fertility problems due to hormonal disruption and optic chiasm compression.</p>
      <p><strong>MRI Features:</strong> Well-defined masses with strong post-contrast enhancement.</p>
    </div>

    <div className="space-y-2">
      <h3 className="text-2xl font-semibold">
        2. Gliomas <span className="text-red-600 text-lg">(Median survival &lt; 15 months)</span>
      </h3>
      <p><strong>Location:</strong> Can appear in cerebral hemispheres, brainstem, or cerebellum.</p>
      <p><strong>Characteristics:</strong> Range from low-grade to aggressive glioblastomas (Grade IV).</p>
      <p><strong>Clinical Impact:</strong> Symptoms depend on location — seizures, personality change, speech difficulties, or weakness. Glioblastomas have a median survival under 15 months.</p>
      <p><strong>MRI Features:</strong> Irregular infiltrative growth with peritumoral edema and mass effect.</p>
    </div>

    <div className="space-y-2">
      <h3 className="text-2xl font-semibold">
        3. Meningiomas <span className="text-yellow-600 text-lg">(Slow-growing, usually noncancerous)</span>
      </h3>
      <p><strong>Location:</strong> Along dural surfaces from arachnoid cap cells (outer brain lining).</p>
      <p><strong>Characteristics:</strong> Most common adult brain tumor, usually benign and slow-growing.</p>
      <p><strong>Clinical Impact:</strong> Symptoms arise from compression, headaches, cognitive changes, seizures depending on tumor position.</p>
      <p><strong>MRI Features:</strong> Homogeneous enhancement with the classic “dural tail” sign.</p>
    </div>
  </div>

  {/* RIGHT: Enlarged Image */}
  <div className="self-center md:w-1/2">
    <CustomImage
      src="/projectImages/mri-CNN/work-1-MRI.png"
      alt="Classified MRI Example"
      className="rounded-lg w-full"
    />
  </div>
</div>

</div>



---

<div className="mt-10 mb-10">

## Why Use CNNs?

<div className="mt-5 mb-10">
Convolutional Neural Networks (CNNs) are designed to analyse visual data. They work by applying filters that detect patterns in an image, starting from <strong>simple edges and shapes</strong>, then building toward <strong>complex features</strong> like tumour textures and outlines.  
This layered pattern recognition makes CNNs particularly effective for <strong>medical image classification</strong>, such as identifying brain tumours from MRI scans.
</div>

## What Are Pretrained Models?

<div className="mt-5 mb-10">
Pretrained models are neural networks that have been previously trained on large and diverse datasets, such as <strong>ImageNet</strong>, which contains over a million natural images across a thousand categories.  
These models have already learned how to extract rich and generalisable visual features like edges, textures, and object parts.

In this project, using pretrained backbones like <strong>Xception</strong>, <strong>MobileNetV2</strong>, and <strong>EfficientNetB0</strong> allows the model to <strong>leverage these learned features</strong> as a starting point, dramatically reducing training time and improving convergence.

Instead of learning from scratch, we <strong>fine-tune</strong> these models on our specific dataset of brain MRI images. This approach is especially useful when working with <strong>limited labelled medical data</strong>, where training a large model from the ground up would be impractical or prone to overfitting.
</div>



---

<div className="flex flex-col items-center my-10">
  <h3 className="text-lg font-semibold mb-4 text-center">Display of MRI Images</h3>
  <img 
    src="/projectImages/mri-CNN/4x4_brain_scans.png" 
    alt="Grid of 4x4 brain MRI scans" 
    className="rounded-xl shadow-lg w-full max-w-4xl"
  />
</div>


---




<div className="mt-10 mb-5">
 <div className ="mb-5">
## Dataset & Scope of Classification
</div>
 
  This project uses a <strong>publicly available brain MRI dataset</strong> that consolidates images from three sources: <strong>Figshare</strong>, <strong>SARTAJ</strong>, and <strong>Br35H</strong>. It contains <strong>7,023 brain MRI images</strong> across four categories: <strong>glioma</strong>, <strong>meningioma</strong>, <strong>pituitary</strong>, and <strong>no tumor</strong>.

  <blockquote className="mt-4">
    Note: The glioma samples from the SARTAJ dataset were found to be mislabeled and were replaced with verified images from Figshare to ensure data integrity.
  </blockquote>
</div>

<div className="flex flex-col md:flex-row justify-center gap-6 mt-8 mb-10">
  <div className="flex-1 text-center">
    <img 
      src="/projectImages/mri-CNN/TrainingSet_Distribution.png" 
      alt="Training set class distribution" 
      className="rounded-lg shadow-md w-full max-w-[400px] mx-auto"
    />
    <p className="mt-3 text-sm">Training Set Distribution</p>
  </div>

  <div className="flex-1 text-center">
    <img 
      src="/projectImages/mri-CNN/TestSet_Distribution.png" 
      alt="Test set class distribution" 
      className="rounded-lg shadow-md w-full max-w-[400px] mx-auto"
    />
    <p className="mt-3 text-sm">Test Set Distribution</p>
  </div>
</div>

<div className="mt-5 mb-10">
  Although the dataset is generally balanced across the four tumour types, the <strong>no tumor</strong> class is slightly overrepresented in both the training and testing sets.  
  This minor imbalance was not sufficient to warrant the use of class weighting or sampling adjustments, but it may influence evaluation metrics such as recall or precision.
</div>


---

<div className="mt-10">
## Preprocessing
</div>




<div className="mt-5 mb-5">

The training set was divided using <strong>stratified sampling</strong> to ensure balanced class representation across training and validation subsets. All images were resized to <code>224×224</code> pixels for input compatibility with pretrained CNNs.

</div>

<div className="mt-5 mb-5">

To enhance generalisation and reduce overfitting, <strong>data augmentation</strong> was applied to the training set using TensorFlow’s <code>ImageDataGenerator</code>, including random rotations (±10°), zoom transformations (±10%), and horizontal flips. Since the dataset included multiple perspectives (axial, sagittal, and coronal), horizontal flipping was deemed appropriate for regularisation.

</div>

<div className="mt-5 mb-5">

Validation and test sets were not augmented and only rescaled to the [0, 1] range, preserving evaluation integrity. This preprocessing approach ensured consistency and robustness across all models during training and evaluation.

</div>

---


<div className = "mt-10 mb-5">
##  Development Environment & Model Approach
</div>




<div className="flex flex-col md:flex-row gap-10 mt-10 mb-10 items-start">

  {/* LEFT: Environment and Model Details */}
  <div className="flex flex-col space-y-6 md:w-1/2">
    <div className="mt-5 mb-5">
      <strong>Compute Environment</strong>: Trained in Google Colab with <strong>Tesla T4 GPU acceleration</strong><br />
      <strong>Model Type</strong>: Deep learning classifier using <strong>Convolutional Neural Networks (CNNs)</strong><br />
      <strong>Pretrained Backbones</strong>: MobileNetV2, EfficientNetB0, and Xception, all initialised with <strong>ImageNet weights</strong><br />
      <strong>Input Dimensions</strong>: All MRI slices resized to <strong>224×224 pixels</strong><br />
      <strong>Frameworks Used</strong>: TensorFlow 2.x with the Keras high-level API
    </div>

    <div>
      All three models, <strong>MobileNetV2</strong>, <strong>EfficientNetB0</strong>, and <strong>Xception</strong>, share the same classification head and training configuration.
      The only difference between them lies in the pretrained backbone used for feature extraction.
    </div>
    <div>
      All input images were resized to <code>224×224</code> pixels for consistency across models.
      However, internal feature map sizes differed based on the architecture.
      For example, <strong>Xception</strong> produced a deeper output feature map of shape <code>(7, 7, 2048)</code> compared to the other models.
    </div>
    <div>
      The following animation visualises the internal structure of each model architecture used in this experiment.
    </div>
  </div>

  {/* RIGHT: Animation */}
  <div className="md:w-1/2 flex justify-center items-center">
    <img 
      src="/projectImages/mri-CNN/ezgif.com-gif-to-webp-converter.webp" 
      alt="Model architecture comparison animation" 
      className="rounded-xl shadow-md w-[350px]"
    />
  </div>

</div>





---

<div className="mt-10">
## Model Performance
</div>


<div className="mt-5 mb-5">

Three pretrained CNN models were evaluated on the brain MRI classification task: **MobileNetV2**, **EfficientNetB0**, and **Xception**. Each model was trained for 10 epochs on the same dataset under identical conditions, allowing for a direct comparison of their learning capabilities and generalisation performance.

</div>

<div className="mb-10">


<h3>Model Performance Summary</h3>

<table style={{ borderCollapse: "collapse", width: "100%", marginTop: "1rem", marginBottom: "1rem" }}>
  <thead>
    <tr>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Model</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Train Accuracy</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Val Accuracy</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Test Accuracy</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Test Precision</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Test Recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Xception</strong></td>
      <td>91.81%</td>
      <td>90.11%</td>
      <td>88.18%</td>
      <td>90.10%</td>
      <td>86.04%</td>
    </tr>
    <tr>
      <td><strong>MobileNetV2</strong></td>
      <td>92.67%</td>
      <td>87.66%</td>
      <td>84.59%</td>
      <td>86.05%</td>
      <td>83.30%</td>
    </tr>
    <tr>
      <td><strong>EfficientNetB0</strong></td>
      <td>27.93%</td>
      <td>27.91%</td>
      <td>30.89%</td>
      <td>0.00%</td>
      <td>0.00%</td>
    </tr>
  </tbody>
</table>

</div>

### Visual Results

<div className="grid grid-cols-2 gap-6 mt-8 mb-10">

  {/* EfficientNetB0 */}
  <div className="text-center">
    <img 
      src="/projectImages/mri-CNN/efficientnet_cm.png"
      alt="EfficientNetB0 Confusion Matrix"
      className="rounded-lg shadow-md w-full max-w-[500px] mx-auto"
    />
    <p className="mt-2 text-sm">EfficientNetB0 Confusion Matrix</p>
  </div>

  <div className="text-center">
    <img 
      src="/projectImages/mri-CNN/efficientnet_training.png"
      alt="EfficientNetB0 Training Metrics"
      className="rounded-lg shadow-md w-full max-w-[750px] mx-auto"
    />
    <p className="mt-2 text-sm">EfficientNetB0 Training Metrics</p>
  </div>

  {/* MobileNetV2 */}
  <div className="text-center">
    <img 
      src="/projectImages/mri-CNN/mobilenet_cm.png"
      alt="MobileNetV2 Confusion Matrix"
      className="rounded-lg shadow-md w-full max-w-[500px] mx-auto"
    />
    <p className="mt-2 text-sm">MobileNetV2 Confusion Matrix</p>
  </div>

  <div className="text-center">
    <img 
      src="/projectImages/mri-CNN/mobilenet_training.png"
      alt="MobileNetV2 Training Metrics"
      className="rounded-lg shadow-md w-full max-w-[750px] mx-auto"
    />
    <p className="mt-2 text-sm">MobileNetV2 Training Metrics</p>
  </div>

  {/* Xception */}
  <div className="text-center">
    <img 
      src="/projectImages/mri-CNN/xception_cm.png"
      alt="Xception Confusion Matrix"
      className="rounded-lg shadow-md w-full max-w-[500px] mx-auto"
    />
    <p className="mt-2 text-sm">Xception Confusion Matrix</p>
  </div>

  <div className="text-center">
    <img 
      src="/projectImages/mri-CNN/xception_training.png"
      alt="Xception Training Metrics"
      className="rounded-lg shadow-md w-full max-w-[750px] mx-auto"
    />
    <p className="mt-2 text-sm">Xception Training Metrics</p>
  </div>

</div>



---

<div className="mt-10">
### Observations
</div>

<div className="mt-5 mb-5">
 <strong>Xception</strong> demonstrated the most robust performance with <code>88.18%</code> test accuracy and excellent generalisation. The minimal gap between training (91.81%) and test accuracy indicates strong model stability. With balanced precision (90.10%) and recall (86.04%), Xception showed consistent classification across all tumor types, making it the most suitable choice for clinical deployment.
</div>

<div className="mt-5 mb-5">
 <strong>MobileNetV2</strong> achieved reasonable performance with <code>84.59%</code> test accuracy but exhibited concerning overfitting patterns. The significant gap between training (92.67%) and validation (87.66%) accuracy suggests limited generalisation capability. While precision remained acceptable at 86.05%, the lower recall (83.30%) indicates potential missed classifications in clinical scenarios.
</div>

<div className="mt-5 mb-5">
 <strong>EfficientNetB0</strong> experienced complete training failure with <code>30.89%</code> test accuracy, performing worse than random classification. The model achieved zero precision and recall, consistently predicting only the "notumor" class regardless of input. This failure highlights the critical importance of base model selection in transfer learning applications and demonstrates that architectural efficiency does not guarantee task suitability.
</div>

<div className="mt-5 mb-5">
 The comparative analysis reveals that model architecture significantly impacts transfer learning effectiveness for medical imaging tasks. <strong>Xception's superior performance</strong> across all metrics establishes it as the optimal choice, while EfficientNetB0's failure provides valuable insights into the limitations of assuming universal model applicability.
</div>





</div>

---


<div className="mt-10 mb-10">
<div className="mb-5">
 ### Consideration 
</div>

While **Xception** demonstrated superior performance in this experiment, it is important to note that with additional hyperparameter tuning, architecture modifications, and training optimisation, both **MobileNetV2** and **EfficientNetB0** could potentially achieve comparable results.

Each pretrained backbone offers unique strengths, **MobileNetV2** for its computational efficiency, **EfficientNetB0** for its compound scaling strategy, and **Xception** for its use of depthwise separable convolutions. These characteristics can be further leveraged through targeted optimisation techniques tailored to each model.


</div>


