import CustomImage from '@/components/CustomImage'
import ModelButton from '@/components/ModelButton'




<div className = "w-1/2 mx-auto text-center mb-5">
# Brain MRI Tumor Classification Using Deep Learning

> üöß **This project is currently in development.**  
> Explore the complete implementation: [GitHub Repository](https://github.com/Brum99/MRI_brain_tumor_prediction)
</div>
---


<div className="mt-10 mb-10">
    <div className="mb-5">
    Why This Matters
    </div>

    Before diving into the technical implementation of this project, it's essential
    to understand **why this work matters**. Brain tumor detection is not just a 
    computational challenge, it‚Äôs a life critical task with real world 
    implications. The types of tumors discussed here differ dramatically in 
    prognosis, treatment approach, and urgency. By establishing a clear clinical 
    picture of these conditions, we can better appreciate the role that machine 
    learning can play in **supporting early, accurate, and scalable diagnosis**.
</div>

---

<div className="mt-10 mb-10">

    <div className="mb-5">
    Clinical Challenge: The Need for Automated Diagnostics
    </div>

    Brain tumors represent one of the most critical diagnostic challenges in modern medicine. These abnormal cell proliferations can be benign or malignant, with early detection being paramount for successful treatment outcomes and patient survival rates.

    <div className="mt-5 mb-5">
    While Magnetic Resonance Imaging (MRI) provides exceptional soft tissue contrast for tumor visualisation, **the manual interpretation process faces significant bottlenecks**:
    </div>

    <div className="mt-5 mb-5">
    - **Time Constraints**: Radiologists may review 100+ scans daily, leading to fatigue-induced oversights
    - **Diagnostic Variability**: Inter-observer agreement rates can vary significantly between practitioners  
    - **Resource Limitations**: Many healthcare systems face critical shortages of specialised radiologists
    - **Urgent Triage Needs**: Emergency cases require rapid preliminary assessments
    </div>


    This project investigates how **Convolutional Neural Networks (CNNs)** can serve as intelligent diagnostic support tools, providing rapid preliminary assessments while maintaining the essential human expertise in final diagnosis.

    <div className="mt-5 mb-5">
    According to the **World Health Organisation (WHO)**, a full clinical diagnosis of brain tumors includes identifying not only the tumor‚Äôs type but also its **grade** and **malignancy** level.  
In this project, we focus **only on classifying the type of tumor** based on MRI features, not on grading or malignancy.
    </div>
This is a crucial step in building automated systems that could eventually aid in faster and more scalable triage, particularly in **resource-limited settings**.

</div>

---

<div className="mt-10 mb-10">
## Tumor Types in This Study

<div className="flex flex-col md:flex-row gap-10 mt-6 items-start">

  {/* LEFT: Tumor Descriptions */}
  <div className="flex flex-col space-y-10 md:w-1/2">
    <div className="space-y-2">
      <h3 className="text-2xl font-semibold">
        1. Pituitary Adenomas <span className="text-green-600 text-lg">(Often benign & highly treatable)</span>
      </h3>
      <p><strong>Location:</strong> Sellar and suprasellar regions, the ‚Äúmaster gland‚Äù area behind the eyes.</p>
      <p><strong>Characteristics:</strong> Benign tumors from the pituitary gland, often hormonally active.</p>
      <p><strong>Clinical Impact:</strong> Can lead to <em>Cushing‚Äôs disease</em>, vision loss, mood swings, and fertility problems due to hormonal disruption and optic chiasm compression.</p>
      <p><strong>MRI Features:</strong> Well-defined masses with strong post-contrast enhancement.</p>
    </div>

    <div className="space-y-2">
      <h3 className="text-2xl font-semibold">
        2. Gliomas <span className="text-red-600 text-lg">(Median survival &lt; 15 months)</span>
      </h3>
      <p><strong>Location:</strong> Can appear in cerebral hemispheres, brainstem, or cerebellum.</p>
      <p><strong>Characteristics:</strong> Range from low-grade to aggressive glioblastomas (Grade IV).</p>
      <p><strong>Clinical Impact:</strong> Symptoms depend on location ‚Äî seizures, personality change, speech difficulties, or weakness. Glioblastomas have a median survival under 15 months.</p>
      <p><strong>MRI Features:</strong> Irregular infiltrative growth with peritumoral edema and mass effect.</p>
    </div>

    <div className="space-y-2">
      <h3 className="text-2xl font-semibold">
        3. Meningiomas <span className="text-yellow-600 text-lg">(Slow-growing, usually noncancerous)</span>
      </h3>
      <p><strong>Location:</strong> Along dural surfaces from arachnoid cap cells (outer brain lining).</p>
      <p><strong>Characteristics:</strong> Most common adult brain tumor, usually benign and slow-growing.</p>
      <p><strong>Clinical Impact:</strong> Symptoms arise from compression, headaches, cognitive changes, seizures depending on tumor position.</p>
      <p><strong>MRI Features:</strong> Homogeneous enhancement with the classic ‚Äúdural tail‚Äù sign.</p>
    </div>
  </div>

  {/* RIGHT: Enlarged Image */}
  <div className="self-center md:w-1/2">
    <CustomImage
      src="/projectImages/mri-CNN/work-1-MRI.png"
      alt="Classified MRI Example"
      className="rounded-lg w-full"
    />
  </div>
</div>

</div>



---

<div className="mt-10 mb-10">

## Why Use CNNs?

<div className="mt-5 mb-10">
Convolutional Neural Networks (CNNs) are designed to analyse visual data. They work by applying filters that detect patterns in an image, starting from <strong>simple edges and shapes</strong>, then building toward <strong>complex features</strong> like tumour textures and outlines.  
This layered pattern recognition makes CNNs particularly effective for <strong>medical image classification</strong>, such as identifying brain tumours from MRI scans.
</div>

## What Are Pretrained Models?

<div className="mt-5 mb-10">
Pretrained models are neural networks that have been previously trained on large and diverse datasets, such as <strong>ImageNet</strong>, which contains over a million natural images across a thousand categories.  
These models have already learned how to extract rich and generalisable visual features like edges, textures, and object parts.

In this project, using pretrained backbones like <strong>Xception</strong>, <strong>MobileNetV2</strong>, and <strong>EfficientNetB0</strong> allows the model to <strong>leverage these learned features</strong> as a starting point, dramatically reducing training time and improving convergence.

Instead of learning from scratch, we <strong>fine-tune</strong> these models on our specific dataset of brain MRI images. This approach is especially useful when working with <strong>limited labelled medical data</strong>, where training a large model from the ground up would be impractical or prone to overfitting.
</div>



---

<div className="flex flex-col items-center my-10">
  <h3 className="text-lg font-semibold mb-4 text-center">Display of MRI Images</h3>
  <img 
    src="/projectImages/mri-CNN/4x4_brain_scans.png" 
    alt="Grid of 4x4 brain MRI scans" 
    className="rounded-xl shadow-lg w-full max-w-4xl"
  />
</div>


---




<div className="mt-10 mb-5">
 <div className ="mb-5">
## Dataset & Scope of Classification
</div>
 
  This project uses a <strong>publicly available brain MRI dataset</strong> that consolidates images from three sources: <strong>Figshare</strong>, <strong>SARTAJ</strong>, and <strong>Br35H</strong>. It contains <strong>7,023 brain MRI images</strong> across four categories: <strong>glioma</strong>, <strong>meningioma</strong>, <strong>pituitary</strong>, and <strong>no tumor</strong>.

  <blockquote className="mt-4">
    Note: The glioma samples from the SARTAJ dataset were found to be mislabeled and were replaced with verified images from Figshare to ensure data integrity.
  </blockquote>
</div>

<div className="flex flex-col md:flex-row justify-center gap-6 mt-8 mb-10">
  <div className="flex-1 text-center">
    <img 
      src="/projectImages/mri-CNN/TrainingSet_Distribution.png" 
      alt="Training set class distribution" 
      className="rounded-lg shadow-md w-full max-w-[400px] mx-auto"
    />
    <p className="mt-3 text-sm">Training Set Distribution</p>
  </div>

  <div className="flex-1 text-center">
    <img 
      src="/projectImages/mri-CNN/TestSet_Distribution.png" 
      alt="Test set class distribution" 
      className="rounded-lg shadow-md w-full max-w-[400px] mx-auto"
    />
    <p className="mt-3 text-sm">Test Set Distribution</p>
  </div>
</div>

<div className="mt-5 mb-10">
  Although the dataset is generally balanced across the four tumour types, the <strong>no tumor</strong> class is slightly overrepresented in both the training and testing sets.  
  This minor imbalance was not sufficient to warrant the use of class weighting or sampling adjustments, but it may influence evaluation metrics such as recall or precision.
</div>


---

<div className="mt-10">
## Preprocessing
</div>




<div className="mt-5 mb-5">

The training set was divided using <strong>stratified sampling</strong> to ensure balanced class representation across training and validation subsets. All images were resized to <code>224√ó224</code> pixels for input compatibility with pretrained CNNs.

</div>

<div className="mt-5 mb-5">

To enhance generalisation and reduce overfitting, <strong>data augmentation</strong> was applied to the training set using TensorFlow‚Äôs <code>ImageDataGenerator</code>, including random rotations (¬±10¬∞), zoom transformations (¬±10%), and horizontal flips. Since the dataset included multiple perspectives (axial, sagittal, and coronal), horizontal flipping was deemed appropriate for regularisation.

</div>

<div className="mt-5 mb-5">

Validation and test sets were not augmented and only rescaled to the [0, 1] range, preserving evaluation integrity. This preprocessing approach ensured consistency and robustness across all models during training and evaluation.

</div>

---


<div className = "mt-10 mb-5">
##  Development Environment & Model Approach
</div>




<div className="flex flex-col md:flex-row gap-10 mt-10 mb-10 items-start">

  {/* LEFT: Environment and Model Details */}
  <div className="flex flex-col space-y-6 md:w-1/2">
    <div className="mt-5 mb-5">
      <strong>Compute Environment</strong>: Trained in Google Colab with <strong>Tesla T4 GPU acceleration</strong><br />
      <strong>Model Type</strong>: Deep learning classifier using <strong>Convolutional Neural Networks (CNNs)</strong><br />
      <strong>Pretrained Backbones</strong>: MobileNetV2, EfficientNetB0, and Xception, all initialised with <strong>ImageNet weights</strong><br />
      <strong>Input Dimensions</strong>: All MRI slices resized to <strong>224√ó224 pixels</strong><br />
      <strong>Frameworks Used</strong>: TensorFlow 2.x with the Keras high-level API
    </div>

    <div>
      All three models, <strong>MobileNetV2</strong>, <strong>EfficientNetB0</strong>, and <strong>Xception</strong>, share the same classification head and training configuration.
      The only difference between them lies in the pretrained backbone used for feature extraction.
    </div>
    <div>
      All input images were resized to <code>224√ó224</code> pixels for consistency across models.
      However, internal feature map sizes differed based on the architecture.
      For example, <strong>Xception</strong> produced a deeper output feature map of shape <code>(7, 7, 2048)</code> compared to the other models.
    </div>
    <div>
      The following animation visualises the internal structure of each model architecture used in this experiment.
    </div>
  </div>

  {/* RIGHT: Animation */}
  <div className="md:w-1/2 flex justify-center items-center">
    <img 
      src="/projectImages/mri-CNN/ezgif.com-gif-to-webp-converter.webp" 
      alt="Model architecture comparison animation" 
      className="rounded-xl shadow-md w-[350px]"
    />
  </div>

</div>





---

<div className="mt-10">
## Model Performance
</div>



<div className="mt-5 mb-5">

Three pretrained CNN models were evaluated on the brain MRI classification task: **MobileNetV2**, **EfficientNetB0**, and **Xception**. Each model was trained for 10 epochs on the same dataset under identical conditions, allowing for a direct comparison of their learning capabilities and generalisation performance.

</div>

<div className="mb-10">


<h3>Model Performance Summary</h3>

<table style={{ borderCollapse: "collapse", width: "100%", marginTop: "1rem", marginBottom: "1rem" }}>
  <thead>
    <tr>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Model</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Train Accuracy</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Val Accuracy</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Test Accuracy</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Test Precision</th>
      <th style={{ textAlign: "left", paddingRight: "1rem" }}>Test Recall</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Xception</strong></td>
      <td>91.81%</td>
      <td>90.11%</td>
      <td>88.18%</td>
      <td>90.10%</td>
      <td>86.04%</td>
    </tr>
    <tr>
      <td><strong>MobileNetV2</strong></td>
      <td>92.67%</td>
      <td>87.66%</td>
      <td>84.59%</td>
      <td>86.05%</td>
      <td>83.30%</td>
    </tr>
    <tr>
      <td><strong>EfficientNetB0</strong></td>
      <td>27.93%</td>
      <td>27.91%</td>
      <td>30.89%</td>
      <td>0.00%</td>
      <td>0.00%</td>
    </tr>
  </tbody>
</table>

</div>

---

<div className="mt-10">
### Observations
</div>



<div className="mt-5 mb-5">

- **Xception** delivered the strongest and most consistent performance across training, validation, and test sets. Its use of depthwise separable convolutions and residual connections proved effective in extracting spatial features from brain MRI scans.
- **MobileNetV2**, despite being a lightweight architecture optimised for efficiency, performed strongly with good training and test metrics. The model showed minor signs of overfitting but maintained reasonable generalisation.
- **EfficientNetB0** failed to train effectively in this configuration. It collapsed to predicting a single class, leading to 0% precision and recall. This may be due to poor initialisation, an unsuitable learning rate, or its sensitivity to regularisation and preprocessing parameters.

</div>

### Visual Results


### Notes

This carousel compares confusion matrices and training metrics across all models.


### Visual Results


### Notes

This carousel compares confusion matrices and training metrics across all models.


---

<div className="mt-5 mb-5">

Training curves for each model (accuracy, loss, precision, recall) are included below. Xception and MobileNetV2 demonstrate stable learning with good convergence and minimal overfitting. In contrast, EfficientNetB0 exhibits stagnant loss and accuracy, confirming its failure to learn useful representations.

</div>

> üìä *Training visualisations:*
> - `xception_training.png`
> - `mobilenet_training.png`
> - `efficientnet_training.png`

</div>



 ### Consideration 
<div className="mt-10 mb-10">

While **Xception** demonstrated superior performance in this experiment, it is important to note that with additional hyperparameter tuning, architecture modifications, and training optimisation, both **MobileNetV2** and **EfficientNetB0** could potentially achieve comparable results.

Each pretrained backbone offers unique strengths, **MobileNetV2** for its computational efficiency, **EfficientNetB0** for its compound scaling strategy, and **Xception** for its use of depthwise separable convolutions. These characteristics can be further leveraged through targeted optimisation techniques tailored to each model.

However, to maintain experimental consistency and based on the results observed under identical training conditions, this project will proceed by further optimising the **Xception** model as it exhibited the most promising baseline performance.

</div>


<div className = "w-1/2 mx-auto text-center mb-5">
### Project In Progress: CNN-Based Brain Tumour Classification  
*Last updated: 23 June 2025*

This project is currently under active development. I am building a deep learning pipeline to classify brain tumours from MRI images using convolutional neural networks (CNNs). The base model is currently implemented using a heavyweight architecture (Xception) and trained on a labelled dataset of glioma, meningioma, pituitary tumours and healthy images.

I am now conducting a series of controlled experiments to investigate how different CNN architectures, lightweight (e.g. MobileNetV2), midweight and heavyweight models, affect classification performance. These experiments also explore the impact of regularisation techniques such as dropout and architectural modifications on model generalisation.

The goal is to produce a robust and well-documented comparison of CNN design choices and their influence on tumour classification accuracy, recall and precision.

Check back soon for updates, results and interactive visualisations.


> üöß **More to come!**  
> Explore the complete implementation: [GitHub Repository](https://github.com/Brum99/MRI_brain_tumor_prediction)
</div>


{/* 

---

<ModelButton endpoint="/api/predict" />

---

##  Performance Analysis & Model Interpretability

<CustomImage src="/projectImages/mri-CNN/work-1-MRI.png" alt="Model classification output with confidence scores" width={600} height={400} />

*Model output demonstrating multi-class probability distributions for tumor classification*

### Gradient-weighted Class Activation Mapping (Grad-CAM)

The visualisation below illustrates our model's attention mechanism, highlighting the specific anatomical regions that influence classification decisions:

<CustomImage src="/images/heatmap.png" alt="Grad-CAM heatmap showing model attention areas" width={600} height={400} />

**Key Observations**:
- Model correctly focuses on tumor boundaries and enhancement patterns
- Attention maps align with radiologically significant features
- Minimal activation in non-pathological brain regions demonstrates specificity

### Performance Metrics by Class

| Tumor Type | Precision | Recall | F1-Score | Support |
|------------|-----------|---------|----------|---------|
| Glioma | 0.89 | 0.94 | 0.91 | 826 |
| Meningioma | 0.95 | 0.88 | 0.91 | 822 |
| Pituitary | 0.93 | 0.91 | 0.92 | 827 |
| **Macro Avg** | **0.92** | **0.91** | **0.91** | **2475** |

---

##  Clinical Validation & Ethical Framework

### Current Limitations
While promising, this research prototype faces several critical limitations that must be addressed before clinical deployment:

**Data Generalisability**: Training data represents a limited demographic and imaging protocol subset. Real-world performance may vary significantly across:
- Different MRI scanner manufacturers and field strengths
- Varying slice thickness and imaging sequences  
- Diverse patient populations and pathology presentations

**Regulatory Compliance**: Medical AI systems require rigorous validation studies and regulatory approval (FDA 510(k) or De Novo pathway) before clinical implementation.

**Integration Challenges**: Seamless PACS/RIS integration, real-time inference capabilities, and radiologist workflow optimisation remain significant hurdles.

### Responsible AI Implementation
**Human-in-the-Loop Design**: AI predictions serve as preliminary assessments, requiring mandatory radiologist review  
**Bias Monitoring**: Continuous evaluation for demographic, technical, and institutional biases  
**Transparency Requirements**: Full explainability of model decisions for clinical accountability  
**Error Analysis**: Comprehensive false positive/negative analysis with clinical correlation

---

##  Potential Clinical Impact

### Immediate Applications
- **Screening Acceleration**: Rapid preliminary assessment in high-volume imaging centers
- **Emergency Triage**: Automated flagging of potentially urgent cases for expedited review
- **Quality Assurance**: Second-opinion validation to reduce diagnostic errors
- **Educational Tool**: Training support for radiology residents and fellows

### Long-term Healthcare Benefits
- **Access Democratisation**: AI-assisted interpretation in underserved regions with limited radiologist availability
- **Cost Optimisation**: Reduced interpretation time and improved diagnostic efficiency
- **Standardisation**: Consistent diagnostic criteria application across different healthcare systems
- **Research Acceleration**: Large-scale retrospective analysis capabilities for clinical research

### Deployment Considerations
Successful clinical integration requires:
- **Multi-institutional validation studies** with diverse patient populations
- **Prospective clinical trials** demonstrating non-inferiority to standard care
- **Continuous learning systems** that adapt to institutional-specific imaging protocols
- **Robust cybersecurity frameworks** protecting sensitive medical data

---

##  Future Research Directions

### Technical Enhancements
- **Multi-modal Integration**: Incorporating diffusion-weighted imaging, perfusion MRI, and MR spectroscopy
- **3D Volumetric Analysis**: Full brain volume assessment rather than 2D slice-based classification
- **Uncertainty Quantification**: Bayesian deep learning approaches for confidence estimation
- **Federated Learning**: Privacy-preserving collaborative training across institutions

### Clinical Validation Pipeline
- **Radiologist Agreement Studies**: Quantifying inter-observer variability and AI performance relative to expert consensus
- **Prospective Clinical Trials**: Real-world validation in clinical workflows
- **Health Economics Analysis**: Cost-effectiveness studies for healthcare system adoption
- **Patient Outcome Correlation**: Long-term follow-up studies linking AI predictions to treatment outcomes

---

##  Key Insights & Contributions

This project demonstrates the significant potential of deep learning in medical imaging while highlighting the complex challenges of responsible AI deployment in healthcare:

**Technical Achievement**: Successfully developed a high-performance CNN achieving 92.3% accuracy across three critical brain tumor types, with explainable AI capabilities essential for clinical trust.

**Clinical Awareness**: Emphasised the importance of human oversight, regulatory compliance, and ethical considerations in medical AI development.

**Research Foundation**: Established a robust framework for future clinical validation studies and real-world deployment considerations.

**Interdisciplinary Approach**: Bridged computer science methodology with clinical neuroscience expertise, demonstrating the collaborative nature required for impactful healthcare AI.

The intersection of artificial intelligence and medical imaging represents one of the most promising frontiers for improving patient outcomes. However, the path from research prototype to clinical implementation requires meticulous attention to safety, efficacy, and ethical considerations that this project has endeavored to address comprehensively.

---

*/}